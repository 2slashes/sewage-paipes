{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KNCPVNLd-r5n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J3Dq2NCK-r5o"
      },
      "outputs": [],
      "source": [
        "class PipesDataset(Dataset):\n",
        "    def __init__(self, path: str):\n",
        "        self.path = path\n",
        "        curr_dir = os.getcwd()\n",
        "        data_path = os.path.join(curr_dir, path)\n",
        "        self.df = pd.read_csv(data_path, dtype=str)  # dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        all = self.df.iloc[idx]\n",
        "\n",
        "        state = all.iloc[0]\n",
        "        action = int(all.iloc[1])\n",
        "\n",
        "        # Create a list, where each entry in the list is an int\n",
        "        # the list as a whole represents the state of the board\n",
        "        state_int_list = [int(x) for x in state]\n",
        "        state_tensor = torch.tensor(state_int_list)\n",
        "\n",
        "        action_tensor = torch.tensor(action)\n",
        "\n",
        "        return (state_tensor, action_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6wOIQPqT-r5o"
      },
      "outputs": [],
      "source": [
        "class PipesPredictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Etp97AOh-r5o"
      },
      "outputs": [],
      "source": [
        "class PipesLoss(nn.Module):\n",
        "    def __init__(self, penalty=1.0):\n",
        "        super().__init__()\n",
        "        self.penalty = penalty\n",
        "\n",
        "    def forward(self, pred, actions):\n",
        "        \"\"\"\n",
        "        predictions: Tensor of shape (batch_size, num_classes) - raw logits\n",
        "        labels: Tensor of shape (batch_size, num_classes) - multi-hot encoded labels (0 or 1)\n",
        "        \"\"\"\n",
        "        # convert the output to probabilities\n",
        "        probabilities = nn.functional.softmax(pred, dim=1)\n",
        "\n",
        "        # normalize labels\n",
        "        normalized_labels = actions / actions.sum(dim=1, keepdim=True).clamp(min=1e-7)\n",
        "\n",
        "        # compute the sum for the labels\n",
        "        sums = torch.sum(actions, dim=1)\n",
        "\n",
        "        # cross entropy loss for each pipe\n",
        "        loss = -torch.sum(normalized_labels * torch.log(probabilities + 1e-7), dim=1) / sums\n",
        "        # take the average of the loss values\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rZCibDph-r5o",
        "outputId": "58ead65e-1f55-4c10-8732-6958a86840aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "# split the data into training and testing data\n",
        "train_data = PipesDataset(\"data/train.csv\")\n",
        "test_data = PipesDataset(\"data/test.csv\")\n",
        "\n",
        "# prepare the dataset for training with DataLoaders\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_data, batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size, shuffle=True)\n",
        "\n",
        "# get the header from the training data\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "\n",
        "test_features, test_labels = next(iter(test_dataloader))\n",
        "\n",
        "device = (\n",
        "    torch.accelerator.current_accelerator().type\n",
        "    if torch.accelerator.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "27zleLJh-r5o"
      },
      "outputs": [],
      "source": [
        "n = 4\n",
        "\n",
        "model = PipesPredictor(n**2 * 4, 64, n**2).to(device)\n",
        "\n",
        "learning_rate = 1e-3\n",
        "epochs = 5\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l8hAgQnx-r5o"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device).float(), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device).float(), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            predicted_labels = torch.argmax(pred, dim=1)\n",
        "            correct += (predicted_labels == y).sum().item()\n",
        "\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(\n",
        "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lsRSPhK-r5o",
        "outputId": "72d0757f-b472-4f6d-9b8d-17b28109af54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------------\n",
            "loss: 2.766629  [   64/2399644]\n",
            "loss: 2.797693  [ 6464/2399644]\n",
            "loss: 2.809934  [12864/2399644]\n",
            "loss: 2.742457  [19264/2399644]\n",
            "loss: 2.753055  [25664/2399644]\n",
            "loss: 2.789501  [32064/2399644]\n",
            "loss: 2.828589  [38464/2399644]\n",
            "loss: 2.764301  [44864/2399644]\n",
            "loss: 2.777147  [51264/2399644]\n",
            "loss: 2.736371  [57664/2399644]\n",
            "loss: 2.762882  [64064/2399644]\n",
            "loss: 2.754668  [70464/2399644]\n",
            "loss: 2.736883  [76864/2399644]\n",
            "loss: 2.750334  [83264/2399644]\n",
            "loss: 2.761255  [89664/2399644]\n",
            "loss: 2.753870  [96064/2399644]\n",
            "loss: 2.769446  [102464/2399644]\n",
            "loss: 2.778872  [108864/2399644]\n",
            "loss: 2.733196  [115264/2399644]\n",
            "loss: 2.731764  [121664/2399644]\n",
            "loss: 2.710052  [128064/2399644]\n",
            "loss: 2.732968  [134464/2399644]\n",
            "loss: 2.737118  [140864/2399644]\n",
            "loss: 2.712777  [147264/2399644]\n",
            "loss: 2.733186  [153664/2399644]\n",
            "loss: 2.713029  [160064/2399644]\n",
            "loss: 2.728734  [166464/2399644]\n",
            "loss: 2.734940  [172864/2399644]\n",
            "loss: 2.723565  [179264/2399644]\n",
            "loss: 2.725061  [185664/2399644]\n",
            "loss: 2.738861  [192064/2399644]\n",
            "loss: 2.712562  [198464/2399644]\n",
            "loss: 2.689855  [204864/2399644]\n",
            "loss: 2.692518  [211264/2399644]\n",
            "loss: 2.705227  [217664/2399644]\n",
            "loss: 2.685456  [224064/2399644]\n",
            "loss: 2.718547  [230464/2399644]\n",
            "loss: 2.690528  [236864/2399644]\n",
            "loss: 2.673884  [243264/2399644]\n",
            "loss: 2.690410  [249664/2399644]\n",
            "loss: 2.683330  [256064/2399644]\n",
            "loss: 2.660343  [262464/2399644]\n",
            "loss: 2.666224  [268864/2399644]\n",
            "loss: 2.726734  [275264/2399644]\n",
            "loss: 2.678299  [281664/2399644]\n",
            "loss: 2.705989  [288064/2399644]\n",
            "loss: 2.660463  [294464/2399644]\n",
            "loss: 2.725216  [300864/2399644]\n",
            "loss: 2.690618  [307264/2399644]\n",
            "loss: 2.685391  [313664/2399644]\n",
            "loss: 2.668920  [320064/2399644]\n",
            "loss: 2.660785  [326464/2399644]\n",
            "loss: 2.641930  [332864/2399644]\n",
            "loss: 2.669042  [339264/2399644]\n",
            "loss: 2.665398  [345664/2399644]\n",
            "loss: 2.640860  [352064/2399644]\n",
            "loss: 2.629855  [358464/2399644]\n",
            "loss: 2.649350  [364864/2399644]\n",
            "loss: 2.645776  [371264/2399644]\n",
            "loss: 2.641160  [377664/2399644]\n",
            "loss: 2.650304  [384064/2399644]\n",
            "loss: 2.626265  [390464/2399644]\n",
            "loss: 2.611289  [396864/2399644]\n",
            "loss: 2.632875  [403264/2399644]\n",
            "loss: 2.652974  [409664/2399644]\n",
            "loss: 2.602098  [416064/2399644]\n",
            "loss: 2.629395  [422464/2399644]\n",
            "loss: 2.627952  [428864/2399644]\n",
            "loss: 2.584451  [435264/2399644]\n",
            "loss: 2.617635  [441664/2399644]\n",
            "loss: 2.641984  [448064/2399644]\n",
            "loss: 2.565761  [454464/2399644]\n",
            "loss: 2.566503  [460864/2399644]\n",
            "loss: 2.583223  [467264/2399644]\n",
            "loss: 2.609223  [473664/2399644]\n",
            "loss: 2.618962  [480064/2399644]\n",
            "loss: 2.575501  [486464/2399644]\n",
            "loss: 2.527921  [492864/2399644]\n",
            "loss: 2.566008  [499264/2399644]\n",
            "loss: 2.569479  [505664/2399644]\n",
            "loss: 2.568833  [512064/2399644]\n",
            "loss: 2.566277  [518464/2399644]\n",
            "loss: 2.570087  [524864/2399644]\n",
            "loss: 2.536275  [531264/2399644]\n",
            "loss: 2.567917  [537664/2399644]\n",
            "loss: 2.530149  [544064/2399644]\n",
            "loss: 2.517513  [550464/2399644]\n",
            "loss: 2.523293  [556864/2399644]\n",
            "loss: 2.517751  [563264/2399644]\n",
            "loss: 2.569744  [569664/2399644]\n",
            "loss: 2.484283  [576064/2399644]\n",
            "loss: 2.541768  [582464/2399644]\n",
            "loss: 2.552195  [588864/2399644]\n",
            "loss: 2.484180  [595264/2399644]\n",
            "loss: 2.551810  [601664/2399644]\n",
            "loss: 2.462938  [608064/2399644]\n",
            "loss: 2.519354  [614464/2399644]\n",
            "loss: 2.504264  [620864/2399644]\n",
            "loss: 2.430055  [627264/2399644]\n",
            "loss: 2.533361  [633664/2399644]\n",
            "loss: 2.443425  [640064/2399644]\n",
            "loss: 2.478988  [646464/2399644]\n",
            "loss: 2.469175  [652864/2399644]\n",
            "loss: 2.460964  [659264/2399644]\n",
            "loss: 2.499204  [665664/2399644]\n",
            "loss: 2.446093  [672064/2399644]\n",
            "loss: 2.488987  [678464/2399644]\n",
            "loss: 2.422163  [684864/2399644]\n",
            "loss: 2.456815  [691264/2399644]\n",
            "loss: 2.435315  [697664/2399644]\n",
            "loss: 2.414166  [704064/2399644]\n",
            "loss: 2.290509  [710464/2399644]\n",
            "loss: 2.339205  [716864/2399644]\n",
            "loss: 2.445319  [723264/2399644]\n",
            "loss: 2.406381  [729664/2399644]\n",
            "loss: 2.380066  [736064/2399644]\n",
            "loss: 2.400513  [742464/2399644]\n",
            "loss: 2.373547  [748864/2399644]\n",
            "loss: 2.379804  [755264/2399644]\n",
            "loss: 2.404295  [761664/2399644]\n",
            "loss: 2.383887  [768064/2399644]\n",
            "loss: 2.331658  [774464/2399644]\n",
            "loss: 2.305349  [780864/2399644]\n",
            "loss: 2.375379  [787264/2399644]\n",
            "loss: 2.366750  [793664/2399644]\n",
            "loss: 2.342272  [800064/2399644]\n",
            "loss: 2.285638  [806464/2399644]\n",
            "loss: 2.333446  [812864/2399644]\n",
            "loss: 2.388962  [819264/2399644]\n",
            "loss: 2.261850  [825664/2399644]\n",
            "loss: 2.271264  [832064/2399644]\n",
            "loss: 2.291467  [838464/2399644]\n",
            "loss: 2.292045  [844864/2399644]\n",
            "loss: 2.313720  [851264/2399644]\n",
            "loss: 2.289225  [857664/2399644]\n",
            "loss: 2.293905  [864064/2399644]\n",
            "loss: 2.271086  [870464/2399644]\n",
            "loss: 2.332704  [876864/2399644]\n",
            "loss: 2.249125  [883264/2399644]\n",
            "loss: 2.320206  [889664/2399644]\n",
            "loss: 2.303455  [896064/2399644]\n",
            "loss: 2.278369  [902464/2399644]\n",
            "loss: 2.261430  [908864/2399644]\n",
            "loss: 2.299616  [915264/2399644]\n",
            "loss: 2.223917  [921664/2399644]\n",
            "loss: 2.300361  [928064/2399644]\n",
            "loss: 2.214546  [934464/2399644]\n",
            "loss: 2.185012  [940864/2399644]\n",
            "loss: 2.240741  [947264/2399644]\n",
            "loss: 2.219655  [953664/2399644]\n",
            "loss: 2.251664  [960064/2399644]\n",
            "loss: 2.174795  [966464/2399644]\n",
            "loss: 2.202999  [972864/2399644]\n",
            "loss: 2.271313  [979264/2399644]\n",
            "loss: 2.222373  [985664/2399644]\n",
            "loss: 2.255723  [992064/2399644]\n",
            "loss: 2.211860  [998464/2399644]\n",
            "loss: 2.287415  [1004864/2399644]\n",
            "loss: 2.190183  [1011264/2399644]\n",
            "loss: 2.203227  [1017664/2399644]\n",
            "loss: 2.267970  [1024064/2399644]\n",
            "loss: 2.136919  [1030464/2399644]\n",
            "loss: 2.172507  [1036864/2399644]\n",
            "loss: 2.245481  [1043264/2399644]\n",
            "loss: 2.278325  [1049664/2399644]\n",
            "loss: 2.240391  [1056064/2399644]\n",
            "loss: 2.170201  [1062464/2399644]\n",
            "loss: 2.130179  [1068864/2399644]\n",
            "loss: 2.157987  [1075264/2399644]\n",
            "loss: 2.141814  [1081664/2399644]\n",
            "loss: 2.111021  [1088064/2399644]\n",
            "loss: 2.227425  [1094464/2399644]\n",
            "loss: 2.119599  [1100864/2399644]\n",
            "loss: 2.071278  [1107264/2399644]\n",
            "loss: 2.090841  [1113664/2399644]\n",
            "loss: 2.063118  [1120064/2399644]\n",
            "loss: 2.180968  [1126464/2399644]\n",
            "loss: 2.148026  [1132864/2399644]\n",
            "loss: 2.100292  [1139264/2399644]\n",
            "loss: 2.045495  [1145664/2399644]\n",
            "loss: 2.083386  [1152064/2399644]\n",
            "loss: 2.092416  [1158464/2399644]\n",
            "loss: 2.149830  [1164864/2399644]\n",
            "loss: 2.168700  [1171264/2399644]\n",
            "loss: 2.212718  [1177664/2399644]\n",
            "loss: 2.051232  [1184064/2399644]\n",
            "loss: 2.092624  [1190464/2399644]\n",
            "loss: 2.029127  [1196864/2399644]\n",
            "loss: 2.099546  [1203264/2399644]\n",
            "loss: 2.068950  [1209664/2399644]\n",
            "loss: 2.083755  [1216064/2399644]\n",
            "loss: 2.048846  [1222464/2399644]\n",
            "loss: 2.054209  [1228864/2399644]\n",
            "loss: 2.163101  [1235264/2399644]\n",
            "loss: 2.066334  [1241664/2399644]\n",
            "loss: 2.084691  [1248064/2399644]\n",
            "loss: 1.962399  [1254464/2399644]\n",
            "loss: 2.121862  [1260864/2399644]\n",
            "loss: 2.074629  [1267264/2399644]\n",
            "loss: 1.978195  [1273664/2399644]\n",
            "loss: 2.000204  [1280064/2399644]\n",
            "loss: 2.082717  [1286464/2399644]\n",
            "loss: 2.022557  [1292864/2399644]\n",
            "loss: 1.959453  [1299264/2399644]\n",
            "loss: 2.067379  [1305664/2399644]\n",
            "loss: 1.953326  [1312064/2399644]\n",
            "loss: 1.971465  [1318464/2399644]\n",
            "loss: 1.980609  [1324864/2399644]\n",
            "loss: 1.987103  [1331264/2399644]\n",
            "loss: 1.985730  [1337664/2399644]\n",
            "loss: 1.888734  [1344064/2399644]\n",
            "loss: 2.041323  [1350464/2399644]\n",
            "loss: 1.975055  [1356864/2399644]\n",
            "loss: 2.113279  [1363264/2399644]\n",
            "loss: 1.932900  [1369664/2399644]\n",
            "loss: 2.004811  [1376064/2399644]\n",
            "loss: 1.904586  [1382464/2399644]\n",
            "loss: 1.850115  [1388864/2399644]\n",
            "loss: 1.993972  [1395264/2399644]\n",
            "loss: 2.019851  [1401664/2399644]\n",
            "loss: 1.940689  [1408064/2399644]\n",
            "loss: 1.904439  [1414464/2399644]\n",
            "loss: 1.958640  [1420864/2399644]\n",
            "loss: 1.886937  [1427264/2399644]\n",
            "loss: 1.927900  [1433664/2399644]\n",
            "loss: 1.838195  [1440064/2399644]\n",
            "loss: 2.004222  [1446464/2399644]\n",
            "loss: 1.951324  [1452864/2399644]\n",
            "loss: 1.896142  [1459264/2399644]\n",
            "loss: 1.872348  [1465664/2399644]\n",
            "loss: 1.939100  [1472064/2399644]\n",
            "loss: 1.897388  [1478464/2399644]\n",
            "loss: 1.962728  [1484864/2399644]\n",
            "loss: 1.833280  [1491264/2399644]\n",
            "loss: 1.941365  [1497664/2399644]\n",
            "loss: 1.911852  [1504064/2399644]\n",
            "loss: 1.916653  [1510464/2399644]\n",
            "loss: 1.883839  [1516864/2399644]\n",
            "loss: 1.973506  [1523264/2399644]\n",
            "loss: 1.837732  [1529664/2399644]\n",
            "loss: 1.818225  [1536064/2399644]\n",
            "loss: 1.821477  [1542464/2399644]\n",
            "loss: 1.895137  [1548864/2399644]\n",
            "loss: 1.760060  [1555264/2399644]\n",
            "loss: 1.822358  [1561664/2399644]\n",
            "loss: 1.789395  [1568064/2399644]\n",
            "loss: 1.909274  [1574464/2399644]\n",
            "loss: 1.786682  [1580864/2399644]\n",
            "loss: 1.899976  [1587264/2399644]\n",
            "loss: 1.823016  [1593664/2399644]\n",
            "loss: 1.843113  [1600064/2399644]\n",
            "loss: 1.825848  [1606464/2399644]\n",
            "loss: 1.961898  [1612864/2399644]\n",
            "loss: 1.785117  [1619264/2399644]\n",
            "loss: 1.907796  [1625664/2399644]\n",
            "loss: 1.751893  [1632064/2399644]\n",
            "loss: 1.747412  [1638464/2399644]\n",
            "loss: 1.935689  [1644864/2399644]\n",
            "loss: 1.959507  [1651264/2399644]\n",
            "loss: 1.880258  [1657664/2399644]\n",
            "loss: 1.821572  [1664064/2399644]\n",
            "loss: 1.794103  [1670464/2399644]\n",
            "loss: 1.768533  [1676864/2399644]\n",
            "loss: 1.887205  [1683264/2399644]\n",
            "loss: 1.710689  [1689664/2399644]\n",
            "loss: 1.818406  [1696064/2399644]\n",
            "loss: 1.832579  [1702464/2399644]\n",
            "loss: 1.809018  [1708864/2399644]\n",
            "loss: 1.694247  [1715264/2399644]\n",
            "loss: 1.811655  [1721664/2399644]\n",
            "loss: 1.803082  [1728064/2399644]\n",
            "loss: 1.834354  [1734464/2399644]\n",
            "loss: 1.815625  [1740864/2399644]\n",
            "loss: 1.774810  [1747264/2399644]\n",
            "loss: 1.977846  [1753664/2399644]\n",
            "loss: 1.733015  [1760064/2399644]\n",
            "loss: 1.804502  [1766464/2399644]\n",
            "loss: 1.764754  [1772864/2399644]\n",
            "loss: 1.782353  [1779264/2399644]\n",
            "loss: 1.843122  [1785664/2399644]\n",
            "loss: 1.697580  [1792064/2399644]\n",
            "loss: 1.759235  [1798464/2399644]\n",
            "loss: 1.826914  [1804864/2399644]\n",
            "loss: 1.748246  [1811264/2399644]\n",
            "loss: 1.682913  [1817664/2399644]\n",
            "loss: 1.750957  [1824064/2399644]\n",
            "loss: 1.713619  [1830464/2399644]\n",
            "loss: 1.726074  [1836864/2399644]\n",
            "loss: 1.838980  [1843264/2399644]\n",
            "loss: 1.631168  [1849664/2399644]\n",
            "loss: 1.701639  [1856064/2399644]\n",
            "loss: 1.701965  [1862464/2399644]\n",
            "loss: 1.682179  [1868864/2399644]\n",
            "loss: 1.780274  [1875264/2399644]\n",
            "loss: 1.716054  [1881664/2399644]\n",
            "loss: 1.717525  [1888064/2399644]\n",
            "loss: 1.807702  [1894464/2399644]\n",
            "loss: 1.671501  [1900864/2399644]\n",
            "loss: 1.862816  [1907264/2399644]\n",
            "loss: 1.686978  [1913664/2399644]\n",
            "loss: 1.679714  [1920064/2399644]\n",
            "loss: 1.821794  [1926464/2399644]\n",
            "loss: 1.591798  [1932864/2399644]\n",
            "loss: 1.646118  [1939264/2399644]\n",
            "loss: 1.767643  [1945664/2399644]\n",
            "loss: 1.683399  [1952064/2399644]\n",
            "loss: 1.738770  [1958464/2399644]\n",
            "loss: 1.743518  [1964864/2399644]\n",
            "loss: 1.755021  [1971264/2399644]\n",
            "loss: 1.712010  [1977664/2399644]\n",
            "loss: 1.729945  [1984064/2399644]\n",
            "loss: 1.732870  [1990464/2399644]\n",
            "loss: 1.636161  [1996864/2399644]\n",
            "loss: 1.729082  [2003264/2399644]\n",
            "loss: 1.694024  [2009664/2399644]\n",
            "loss: 1.695091  [2016064/2399644]\n",
            "loss: 1.673819  [2022464/2399644]\n",
            "loss: 1.716823  [2028864/2399644]\n",
            "loss: 1.704530  [2035264/2399644]\n",
            "loss: 1.706720  [2041664/2399644]\n",
            "loss: 1.620250  [2048064/2399644]\n",
            "loss: 1.621910  [2054464/2399644]\n",
            "loss: 1.616201  [2060864/2399644]\n",
            "loss: 1.640626  [2067264/2399644]\n",
            "loss: 1.707161  [2073664/2399644]\n",
            "loss: 1.626995  [2080064/2399644]\n",
            "loss: 1.662746  [2086464/2399644]\n",
            "loss: 1.581732  [2092864/2399644]\n",
            "loss: 1.614361  [2099264/2399644]\n",
            "loss: 1.615617  [2105664/2399644]\n",
            "loss: 1.574457  [2112064/2399644]\n",
            "loss: 1.650420  [2118464/2399644]\n",
            "loss: 1.619400  [2124864/2399644]\n",
            "loss: 1.767016  [2131264/2399644]\n",
            "loss: 1.524849  [2137664/2399644]\n",
            "loss: 1.595372  [2144064/2399644]\n",
            "loss: 1.742646  [2150464/2399644]\n",
            "loss: 1.732668  [2156864/2399644]\n",
            "loss: 1.634920  [2163264/2399644]\n",
            "loss: 1.538049  [2169664/2399644]\n",
            "loss: 1.691028  [2176064/2399644]\n",
            "loss: 1.595953  [2182464/2399644]\n",
            "loss: 1.547238  [2188864/2399644]\n",
            "loss: 1.618958  [2195264/2399644]\n",
            "loss: 1.905220  [2201664/2399644]\n",
            "loss: 1.522730  [2208064/2399644]\n",
            "loss: 1.659513  [2214464/2399644]\n",
            "loss: 1.485686  [2220864/2399644]\n",
            "loss: 1.583408  [2227264/2399644]\n",
            "loss: 1.666180  [2233664/2399644]\n",
            "loss: 1.657717  [2240064/2399644]\n",
            "loss: 1.732624  [2246464/2399644]\n",
            "loss: 1.671159  [2252864/2399644]\n",
            "loss: 1.598147  [2259264/2399644]\n",
            "loss: 1.526670  [2265664/2399644]\n",
            "loss: 1.640862  [2272064/2399644]\n",
            "loss: 1.564524  [2278464/2399644]\n",
            "loss: 1.647186  [2284864/2399644]\n",
            "loss: 1.593231  [2291264/2399644]\n",
            "loss: 1.677776  [2297664/2399644]\n",
            "loss: 1.511501  [2304064/2399644]\n",
            "loss: 1.683061  [2310464/2399644]\n",
            "loss: 1.725516  [2316864/2399644]\n",
            "loss: 1.558494  [2323264/2399644]\n",
            "loss: 1.374669  [2329664/2399644]\n",
            "loss: 1.586225  [2336064/2399644]\n",
            "loss: 1.520870  [2342464/2399644]\n",
            "loss: 1.561809  [2348864/2399644]\n",
            "loss: 1.501113  [2355264/2399644]\n",
            "loss: 1.686339  [2361664/2399644]\n",
            "loss: 1.593970  [2368064/2399644]\n",
            "loss: 1.663011  [2374464/2399644]\n",
            "loss: 1.572991  [2380864/2399644]\n",
            "loss: 1.459644  [2387264/2399644]\n",
            "loss: 1.649587  [2393664/2399644]\n",
            "Test Error: \n",
            " Accuracy: 51.9%, Avg loss: 1.641695 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------------\n",
            "loss: 1.425773  [   64/2399644]\n",
            "loss: 1.439979  [ 6464/2399644]\n",
            "loss: 1.600278  [12864/2399644]\n",
            "loss: 1.564018  [19264/2399644]\n",
            "loss: 1.511652  [25664/2399644]\n",
            "loss: 1.706490  [32064/2399644]\n",
            "loss: 1.555481  [38464/2399644]\n",
            "loss: 1.588820  [44864/2399644]\n",
            "loss: 1.634556  [51264/2399644]\n",
            "loss: 1.627071  [57664/2399644]\n",
            "loss: 1.449278  [64064/2399644]\n",
            "loss: 1.576050  [70464/2399644]\n",
            "loss: 1.500984  [76864/2399644]\n",
            "loss: 1.511013  [83264/2399644]\n",
            "loss: 1.453390  [89664/2399644]\n",
            "loss: 1.534713  [96064/2399644]\n",
            "loss: 1.568782  [102464/2399644]\n",
            "loss: 1.435773  [108864/2399644]\n",
            "loss: 1.545444  [115264/2399644]\n",
            "loss: 1.281017  [121664/2399644]\n",
            "loss: 1.617525  [128064/2399644]\n",
            "loss: 1.671302  [134464/2399644]\n",
            "loss: 1.631063  [140864/2399644]\n",
            "loss: 1.577450  [147264/2399644]\n",
            "loss: 1.520360  [153664/2399644]\n",
            "loss: 1.575698  [160064/2399644]\n",
            "loss: 1.614503  [166464/2399644]\n",
            "loss: 1.614022  [172864/2399644]\n",
            "loss: 1.548387  [179264/2399644]\n",
            "loss: 1.585749  [185664/2399644]\n",
            "loss: 1.456337  [192064/2399644]\n",
            "loss: 1.424937  [198464/2399644]\n",
            "loss: 1.560448  [204864/2399644]\n",
            "loss: 1.574085  [211264/2399644]\n",
            "loss: 1.310355  [217664/2399644]\n",
            "loss: 1.551100  [224064/2399644]\n",
            "loss: 1.537142  [230464/2399644]\n",
            "loss: 1.577851  [236864/2399644]\n",
            "loss: 1.541851  [243264/2399644]\n",
            "loss: 1.391678  [249664/2399644]\n",
            "loss: 1.514337  [256064/2399644]\n",
            "loss: 1.528895  [262464/2399644]\n",
            "loss: 1.437282  [268864/2399644]\n",
            "loss: 1.528443  [275264/2399644]\n",
            "loss: 1.489537  [281664/2399644]\n",
            "loss: 1.484905  [288064/2399644]\n",
            "loss: 1.603801  [294464/2399644]\n",
            "loss: 1.529695  [300864/2399644]\n",
            "loss: 1.552422  [307264/2399644]\n",
            "loss: 1.718147  [313664/2399644]\n",
            "loss: 1.547720  [320064/2399644]\n",
            "loss: 1.605939  [326464/2399644]\n",
            "loss: 1.690912  [332864/2399644]\n",
            "loss: 1.585774  [339264/2399644]\n",
            "loss: 1.543988  [345664/2399644]\n",
            "loss: 1.443277  [352064/2399644]\n",
            "loss: 1.504896  [358464/2399644]\n",
            "loss: 1.489913  [364864/2399644]\n",
            "loss: 1.600900  [371264/2399644]\n",
            "loss: 1.461948  [377664/2399644]\n",
            "loss: 1.491675  [384064/2399644]\n",
            "loss: 1.439455  [390464/2399644]\n",
            "loss: 1.418938  [396864/2399644]\n",
            "loss: 1.635758  [403264/2399644]\n",
            "loss: 1.457123  [409664/2399644]\n",
            "loss: 1.464786  [416064/2399644]\n",
            "loss: 1.487076  [422464/2399644]\n",
            "loss: 1.384492  [428864/2399644]\n",
            "loss: 1.363421  [435264/2399644]\n",
            "loss: 1.439215  [441664/2399644]\n",
            "loss: 1.558747  [448064/2399644]\n",
            "loss: 1.556967  [454464/2399644]\n",
            "loss: 1.482525  [460864/2399644]\n",
            "loss: 1.450223  [467264/2399644]\n",
            "loss: 1.360555  [473664/2399644]\n",
            "loss: 1.279365  [480064/2399644]\n",
            "loss: 1.444784  [486464/2399644]\n",
            "loss: 1.543653  [492864/2399644]\n",
            "loss: 1.414592  [499264/2399644]\n",
            "loss: 1.340357  [505664/2399644]\n",
            "loss: 1.281029  [512064/2399644]\n",
            "loss: 1.393734  [518464/2399644]\n",
            "loss: 1.434981  [524864/2399644]\n",
            "loss: 1.479560  [531264/2399644]\n",
            "loss: 1.378587  [537664/2399644]\n",
            "loss: 1.539664  [544064/2399644]\n",
            "loss: 1.482774  [550464/2399644]\n",
            "loss: 1.520800  [556864/2399644]\n",
            "loss: 1.387136  [563264/2399644]\n",
            "loss: 1.391985  [569664/2399644]\n",
            "loss: 1.492435  [576064/2399644]\n",
            "loss: 1.485496  [582464/2399644]\n",
            "loss: 1.526163  [588864/2399644]\n",
            "loss: 1.507217  [595264/2399644]\n",
            "loss: 1.390388  [601664/2399644]\n",
            "loss: 1.253278  [608064/2399644]\n",
            "loss: 1.452108  [614464/2399644]\n",
            "loss: 1.479772  [620864/2399644]\n",
            "loss: 1.512257  [627264/2399644]\n",
            "loss: 1.536659  [633664/2399644]\n",
            "loss: 1.487669  [640064/2399644]\n",
            "loss: 1.592168  [646464/2399644]\n",
            "loss: 1.477568  [652864/2399644]\n",
            "loss: 1.334977  [659264/2399644]\n",
            "loss: 1.505554  [665664/2399644]\n",
            "loss: 1.323449  [672064/2399644]\n",
            "loss: 1.333598  [678464/2399644]\n",
            "loss: 1.154508  [684864/2399644]\n",
            "loss: 1.537075  [691264/2399644]\n",
            "loss: 1.479589  [697664/2399644]\n",
            "loss: 1.382891  [704064/2399644]\n",
            "loss: 1.386275  [710464/2399644]\n",
            "loss: 1.407833  [716864/2399644]\n",
            "loss: 1.364377  [723264/2399644]\n",
            "loss: 1.693147  [729664/2399644]\n",
            "loss: 1.423014  [736064/2399644]\n",
            "loss: 1.337502  [742464/2399644]\n",
            "loss: 1.429036  [748864/2399644]\n",
            "loss: 1.466875  [755264/2399644]\n",
            "loss: 1.390776  [761664/2399644]\n",
            "loss: 1.538687  [768064/2399644]\n",
            "loss: 1.366263  [774464/2399644]\n",
            "loss: 1.557772  [780864/2399644]\n",
            "loss: 1.560760  [787264/2399644]\n",
            "loss: 1.433066  [793664/2399644]\n",
            "loss: 1.383788  [800064/2399644]\n",
            "loss: 1.327678  [806464/2399644]\n",
            "loss: 1.288822  [812864/2399644]\n",
            "loss: 1.484395  [819264/2399644]\n",
            "loss: 1.385261  [825664/2399644]\n",
            "loss: 1.403011  [832064/2399644]\n",
            "loss: 1.434458  [838464/2399644]\n",
            "loss: 1.442722  [844864/2399644]\n",
            "loss: 1.509387  [851264/2399644]\n",
            "loss: 1.254902  [857664/2399644]\n",
            "loss: 1.455193  [864064/2399644]\n",
            "loss: 1.613301  [870464/2399644]\n",
            "loss: 1.354841  [876864/2399644]\n",
            "loss: 1.485329  [883264/2399644]\n",
            "loss: 1.427097  [889664/2399644]\n",
            "loss: 1.439059  [896064/2399644]\n",
            "loss: 1.365944  [902464/2399644]\n",
            "loss: 1.481090  [908864/2399644]\n",
            "loss: 1.509152  [915264/2399644]\n",
            "loss: 1.585094  [921664/2399644]\n",
            "loss: 1.524602  [928064/2399644]\n",
            "loss: 1.544723  [934464/2399644]\n",
            "loss: 1.428503  [940864/2399644]\n",
            "loss: 1.402204  [947264/2399644]\n",
            "loss: 1.418695  [953664/2399644]\n",
            "loss: 1.453445  [960064/2399644]\n",
            "loss: 1.510054  [966464/2399644]\n",
            "loss: 1.415785  [972864/2399644]\n",
            "loss: 1.313445  [979264/2399644]\n",
            "loss: 1.326581  [985664/2399644]\n",
            "loss: 1.478474  [992064/2399644]\n",
            "loss: 1.337155  [998464/2399644]\n",
            "loss: 1.271463  [1004864/2399644]\n",
            "loss: 1.432529  [1011264/2399644]\n",
            "loss: 1.514717  [1017664/2399644]\n",
            "loss: 1.424495  [1024064/2399644]\n",
            "loss: 1.368783  [1030464/2399644]\n",
            "loss: 1.336067  [1036864/2399644]\n",
            "loss: 1.353134  [1043264/2399644]\n",
            "loss: 1.387303  [1049664/2399644]\n",
            "loss: 1.431439  [1056064/2399644]\n",
            "loss: 1.489222  [1062464/2399644]\n",
            "loss: 1.286723  [1068864/2399644]\n",
            "loss: 1.467569  [1075264/2399644]\n",
            "loss: 1.383179  [1081664/2399644]\n",
            "loss: 1.497767  [1088064/2399644]\n",
            "loss: 1.260854  [1094464/2399644]\n",
            "loss: 1.290329  [1100864/2399644]\n",
            "loss: 1.458083  [1107264/2399644]\n",
            "loss: 1.292929  [1113664/2399644]\n",
            "loss: 1.559199  [1120064/2399644]\n",
            "loss: 1.458484  [1126464/2399644]\n",
            "loss: 1.423334  [1132864/2399644]\n",
            "loss: 1.296729  [1139264/2399644]\n",
            "loss: 1.422917  [1145664/2399644]\n",
            "loss: 1.285568  [1152064/2399644]\n",
            "loss: 1.242153  [1158464/2399644]\n",
            "loss: 1.261965  [1164864/2399644]\n",
            "loss: 1.416529  [1171264/2399644]\n",
            "loss: 1.309606  [1177664/2399644]\n",
            "loss: 1.353945  [1184064/2399644]\n",
            "loss: 1.226581  [1190464/2399644]\n",
            "loss: 1.389196  [1196864/2399644]\n",
            "loss: 1.290653  [1203264/2399644]\n",
            "loss: 1.352197  [1209664/2399644]\n",
            "loss: 1.310313  [1216064/2399644]\n",
            "loss: 1.246616  [1222464/2399644]\n",
            "loss: 1.312133  [1228864/2399644]\n",
            "loss: 1.471563  [1235264/2399644]\n",
            "loss: 1.274594  [1241664/2399644]\n",
            "loss: 1.424694  [1248064/2399644]\n",
            "loss: 1.350947  [1254464/2399644]\n",
            "loss: 1.401656  [1260864/2399644]\n",
            "loss: 1.231444  [1267264/2399644]\n",
            "loss: 1.466552  [1273664/2399644]\n",
            "loss: 1.280175  [1280064/2399644]\n",
            "loss: 1.131960  [1286464/2399644]\n",
            "loss: 1.299752  [1292864/2399644]\n",
            "loss: 1.454732  [1299264/2399644]\n",
            "loss: 1.453182  [1305664/2399644]\n",
            "loss: 1.414963  [1312064/2399644]\n",
            "loss: 1.466865  [1318464/2399644]\n",
            "loss: 1.340908  [1324864/2399644]\n",
            "loss: 1.547323  [1331264/2399644]\n",
            "loss: 1.320909  [1337664/2399644]\n",
            "loss: 1.468486  [1344064/2399644]\n",
            "loss: 1.270959  [1350464/2399644]\n",
            "loss: 1.359493  [1356864/2399644]\n",
            "loss: 1.341833  [1363264/2399644]\n",
            "loss: 1.272693  [1369664/2399644]\n",
            "loss: 1.374484  [1376064/2399644]\n",
            "loss: 1.226370  [1382464/2399644]\n",
            "loss: 1.384750  [1388864/2399644]\n",
            "loss: 1.289855  [1395264/2399644]\n",
            "loss: 1.184566  [1401664/2399644]\n",
            "loss: 1.322868  [1408064/2399644]\n",
            "loss: 1.160030  [1414464/2399644]\n",
            "loss: 1.148916  [1420864/2399644]\n",
            "loss: 1.350253  [1427264/2399644]\n",
            "loss: 1.397050  [1433664/2399644]\n",
            "loss: 1.229508  [1440064/2399644]\n",
            "loss: 1.238798  [1446464/2399644]\n",
            "loss: 1.347174  [1452864/2399644]\n",
            "loss: 1.311199  [1459264/2399644]\n",
            "loss: 1.200847  [1465664/2399644]\n",
            "loss: 1.516003  [1472064/2399644]\n",
            "loss: 1.185812  [1478464/2399644]\n",
            "loss: 1.405844  [1484864/2399644]\n",
            "loss: 1.231930  [1491264/2399644]\n",
            "loss: 1.169844  [1497664/2399644]\n",
            "loss: 1.238998  [1504064/2399644]\n",
            "loss: 1.411329  [1510464/2399644]\n",
            "loss: 1.129881  [1516864/2399644]\n",
            "loss: 1.161334  [1523264/2399644]\n",
            "loss: 1.407157  [1529664/2399644]\n",
            "loss: 1.362000  [1536064/2399644]\n",
            "loss: 1.213655  [1542464/2399644]\n",
            "loss: 1.261276  [1548864/2399644]\n",
            "loss: 1.409109  [1555264/2399644]\n",
            "loss: 1.190717  [1561664/2399644]\n",
            "loss: 1.221909  [1568064/2399644]\n",
            "loss: 1.160855  [1574464/2399644]\n",
            "loss: 1.254696  [1580864/2399644]\n",
            "loss: 1.366680  [1587264/2399644]\n",
            "loss: 1.250758  [1593664/2399644]\n",
            "loss: 1.166761  [1600064/2399644]\n",
            "loss: 1.209599  [1606464/2399644]\n",
            "loss: 1.391205  [1612864/2399644]\n",
            "loss: 1.323480  [1619264/2399644]\n",
            "loss: 1.231724  [1625664/2399644]\n",
            "loss: 1.410354  [1632064/2399644]\n",
            "loss: 1.213355  [1638464/2399644]\n",
            "loss: 1.284734  [1644864/2399644]\n",
            "loss: 1.315041  [1651264/2399644]\n",
            "loss: 1.285559  [1657664/2399644]\n",
            "loss: 1.223080  [1664064/2399644]\n",
            "loss: 1.539762  [1670464/2399644]\n",
            "loss: 1.281518  [1676864/2399644]\n",
            "loss: 1.174000  [1683264/2399644]\n",
            "loss: 1.408369  [1689664/2399644]\n",
            "loss: 1.367374  [1696064/2399644]\n",
            "loss: 1.307065  [1702464/2399644]\n",
            "loss: 1.287500  [1708864/2399644]\n",
            "loss: 1.416998  [1715264/2399644]\n",
            "loss: 1.223624  [1721664/2399644]\n",
            "loss: 1.274296  [1728064/2399644]\n",
            "loss: 1.382002  [1734464/2399644]\n",
            "loss: 1.300131  [1740864/2399644]\n",
            "loss: 1.278013  [1747264/2399644]\n",
            "loss: 1.449879  [1753664/2399644]\n",
            "loss: 1.312624  [1760064/2399644]\n",
            "loss: 1.332331  [1766464/2399644]\n",
            "loss: 1.269904  [1772864/2399644]\n",
            "loss: 1.271083  [1779264/2399644]\n",
            "loss: 1.328912  [1785664/2399644]\n",
            "loss: 1.193244  [1792064/2399644]\n",
            "loss: 1.317791  [1798464/2399644]\n",
            "loss: 1.276879  [1804864/2399644]\n",
            "loss: 1.267135  [1811264/2399644]\n",
            "loss: 1.366653  [1817664/2399644]\n",
            "loss: 1.354258  [1824064/2399644]\n",
            "loss: 1.298859  [1830464/2399644]\n",
            "loss: 1.271055  [1836864/2399644]\n",
            "loss: 1.323936  [1843264/2399644]\n",
            "loss: 1.297809  [1849664/2399644]\n",
            "loss: 1.137334  [1856064/2399644]\n",
            "loss: 1.347258  [1862464/2399644]\n",
            "loss: 1.480939  [1868864/2399644]\n",
            "loss: 1.276505  [1875264/2399644]\n",
            "loss: 1.389831  [1881664/2399644]\n",
            "loss: 1.364294  [1888064/2399644]\n",
            "loss: 1.154117  [1894464/2399644]\n",
            "loss: 1.168854  [1900864/2399644]\n",
            "loss: 1.227251  [1907264/2399644]\n",
            "loss: 1.205469  [1913664/2399644]\n",
            "loss: 1.256301  [1920064/2399644]\n",
            "loss: 1.228691  [1926464/2399644]\n",
            "loss: 1.347437  [1932864/2399644]\n",
            "loss: 1.222668  [1939264/2399644]\n",
            "loss: 1.175015  [1945664/2399644]\n",
            "loss: 1.258484  [1952064/2399644]\n",
            "loss: 1.228442  [1958464/2399644]\n",
            "loss: 1.226373  [1964864/2399644]\n",
            "loss: 1.312835  [1971264/2399644]\n",
            "loss: 1.408793  [1977664/2399644]\n",
            "loss: 1.160872  [1984064/2399644]\n",
            "loss: 1.228348  [1990464/2399644]\n",
            "loss: 1.250962  [1996864/2399644]\n",
            "loss: 1.474709  [2003264/2399644]\n",
            "loss: 1.380653  [2009664/2399644]\n",
            "loss: 1.201738  [2016064/2399644]\n",
            "loss: 1.329313  [2022464/2399644]\n",
            "loss: 1.325130  [2028864/2399644]\n",
            "loss: 1.144844  [2035264/2399644]\n",
            "loss: 1.452158  [2041664/2399644]\n",
            "loss: 1.301389  [2048064/2399644]\n",
            "loss: 1.251863  [2054464/2399644]\n",
            "loss: 1.344766  [2060864/2399644]\n",
            "loss: 1.438244  [2067264/2399644]\n",
            "loss: 1.339689  [2073664/2399644]\n",
            "loss: 1.251600  [2080064/2399644]\n",
            "loss: 1.143151  [2086464/2399644]\n",
            "loss: 1.401880  [2092864/2399644]\n",
            "loss: 1.212922  [2099264/2399644]\n",
            "loss: 1.246775  [2105664/2399644]\n",
            "loss: 1.254401  [2112064/2399644]\n",
            "loss: 1.218640  [2118464/2399644]\n",
            "loss: 1.382665  [2124864/2399644]\n",
            "loss: 1.245064  [2131264/2399644]\n",
            "loss: 1.306901  [2137664/2399644]\n",
            "loss: 1.201356  [2144064/2399644]\n",
            "loss: 1.274392  [2150464/2399644]\n",
            "loss: 1.273805  [2156864/2399644]\n",
            "loss: 1.288553  [2163264/2399644]\n",
            "loss: 1.229825  [2169664/2399644]\n",
            "loss: 1.247563  [2176064/2399644]\n",
            "loss: 1.183294  [2182464/2399644]\n",
            "loss: 1.155864  [2188864/2399644]\n",
            "loss: 1.168627  [2195264/2399644]\n",
            "loss: 1.414039  [2201664/2399644]\n",
            "loss: 1.253358  [2208064/2399644]\n",
            "loss: 1.492654  [2214464/2399644]\n",
            "loss: 1.330047  [2220864/2399644]\n",
            "loss: 1.360251  [2227264/2399644]\n",
            "loss: 1.228197  [2233664/2399644]\n",
            "loss: 1.360858  [2240064/2399644]\n",
            "loss: 1.105824  [2246464/2399644]\n",
            "loss: 1.232325  [2252864/2399644]\n",
            "loss: 1.224715  [2259264/2399644]\n",
            "loss: 1.203400  [2265664/2399644]\n",
            "loss: 1.169511  [2272064/2399644]\n",
            "loss: 1.440006  [2278464/2399644]\n",
            "loss: 1.227460  [2284864/2399644]\n",
            "loss: 1.204139  [2291264/2399644]\n",
            "loss: 1.225650  [2297664/2399644]\n",
            "loss: 1.445952  [2304064/2399644]\n",
            "loss: 1.259028  [2310464/2399644]\n",
            "loss: 1.227805  [2316864/2399644]\n",
            "loss: 1.449116  [2323264/2399644]\n",
            "loss: 1.390041  [2329664/2399644]\n",
            "loss: 1.170480  [2336064/2399644]\n",
            "loss: 1.374791  [2342464/2399644]\n",
            "loss: 1.241885  [2348864/2399644]\n",
            "loss: 1.339996  [2355264/2399644]\n",
            "loss: 1.357479  [2361664/2399644]\n",
            "loss: 1.190136  [2368064/2399644]\n",
            "loss: 1.388900  [2374464/2399644]\n",
            "loss: 1.144935  [2380864/2399644]\n",
            "loss: 1.275485  [2387264/2399644]\n",
            "loss: 1.189106  [2393664/2399644]\n",
            "Test Error: \n",
            " Accuracy: 56.3%, Avg loss: 1.343408 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------------\n",
            "loss: 1.344245  [   64/2399644]\n",
            "loss: 1.489911  [ 6464/2399644]\n",
            "loss: 1.219077  [12864/2399644]\n",
            "loss: 1.304723  [19264/2399644]\n",
            "loss: 1.195010  [25664/2399644]\n",
            "loss: 1.047628  [32064/2399644]\n",
            "loss: 1.096400  [38464/2399644]\n",
            "loss: 1.368439  [44864/2399644]\n",
            "loss: 1.290534  [51264/2399644]\n",
            "loss: 1.074486  [57664/2399644]\n",
            "loss: 1.165721  [64064/2399644]\n",
            "loss: 1.229638  [70464/2399644]\n",
            "loss: 1.354066  [76864/2399644]\n",
            "loss: 1.132502  [83264/2399644]\n",
            "loss: 1.427502  [89664/2399644]\n",
            "loss: 1.229328  [96064/2399644]\n",
            "loss: 1.424666  [102464/2399644]\n",
            "loss: 1.154394  [108864/2399644]\n",
            "loss: 1.424655  [115264/2399644]\n",
            "loss: 1.384597  [121664/2399644]\n",
            "loss: 1.385750  [128064/2399644]\n",
            "loss: 1.287325  [134464/2399644]\n",
            "loss: 1.217120  [140864/2399644]\n",
            "loss: 1.245933  [147264/2399644]\n",
            "loss: 1.232779  [153664/2399644]\n",
            "loss: 1.223956  [160064/2399644]\n",
            "loss: 1.153337  [166464/2399644]\n",
            "loss: 1.331966  [172864/2399644]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()  # Start timer\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "end_time = time.time()  # End timer\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f\"Total time: {elapsed_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAV9K-Gt-r5o"
      },
      "outputs": [],
      "source": [
        "model_file_name = \"model.pth\"\n",
        "curr_dir = os.getcwd()\n",
        "model_file_path = os.path.join(curr_dir, model_file_name)\n",
        "torch.save(model.state_dict(), model_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrGGAfHx-r5p"
      },
      "outputs": [],
      "source": [
        "model_file_name = \"model.pth\"\n",
        "model_file_path = os.path.join(curr_dir, model_file_name)\n",
        "model.load_state_dict(torch.load(model_file_path, weights_only=True))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tVTUOqI-r5p"
      },
      "outputs": [],
      "source": [
        "global cache\n",
        "cache = []\n",
        "global visited\n",
        "visited = set()\n",
        "\n",
        "\n",
        "# initial = \"0010100001001001101011001010011101011000010001111001110101011100\"\n",
        "# goal = \"0010001001000011101011000101101110100010001010111100110101011001\"\n",
        "initial = \"0110101010110100010101101110000110101010001110111000001101000100\"\n",
        "goal = \"0110010101110001101001101011001010101010110010111000110000011000\"\n",
        "# state_int_list = [int(x) for x in state]\n",
        "# state_tensor = torch.tensor(state_int_list).to(device).float()\n",
        "# result = model(state_tensor)\n",
        "# print(torch.softmax(result, dim=0))\n",
        "# print(torch.argmax(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDUXoaWJ-r5p"
      },
      "outputs": [],
      "source": [
        "def pick_move(state):\n",
        "    global cache\n",
        "    global visited\n",
        "    if len(cache) > 3:\n",
        "        cache = cache[-3:]\n",
        "    # convert the state to integers\n",
        "    state_int_list = [int(x) for x in state]\n",
        "    # convert the state to a tensor\n",
        "    state_tensor = torch.tensor(state_int_list).to(device).float()\n",
        "    # get the predicted move from the neural network\n",
        "    prob = model(state_tensor)\n",
        "    # result = torch.argmax(prob).item()\n",
        "    results = torch.topk(prob, 16).indices\n",
        "    # print(results)\n",
        "    # output the result\n",
        "    # print(torch.softmax(prob, dim=0))\n",
        "    # print(result)\n",
        "    # cache.append(result)\n",
        "\n",
        "    # apply the rotation to the state and return the new state\n",
        "    new_state = state\n",
        "    result = 0\n",
        "    for r in results:\n",
        "        result = int(r)\n",
        "        new_state = pipe_rotate_binary(result, state)\n",
        "        if new_state not in visited:\n",
        "            visited.add(new_state)\n",
        "            break\n",
        "    if new_state == state:\n",
        "        raise Exception(\"chyme\")\n",
        "    return new_state, result\n",
        "\n",
        "\n",
        "def pipe_rotate_binary(pipe: int, board: str):\n",
        "    \"\"\"\n",
        "    Takes a binary representation of a board of pipes as a string, and a pipe to rotate. Outputs a binary representation of the board after rotating the pipe.\n",
        "\n",
        "    :params pipe: The pipe to rotate\n",
        "    :params board: Binary representation of the board as a string\n",
        "\n",
        "    \"\"\"\n",
        "    # each pipe has 4 values associated to it, so pipe n starts at index 4 * n\n",
        "    start_index = 4 * pipe\n",
        "    up = board[start_index]\n",
        "    right = board[start_index + 1]\n",
        "    down = board[start_index + 2]\n",
        "    left = board[start_index + 3]\n",
        "\n",
        "    # rotate clockwise\n",
        "    new_board = (\n",
        "        board[:start_index] + left + up + right + down + board[start_index + 4 :]\n",
        "    )\n",
        "\n",
        "    return new_board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-TWxwEa-r5p"
      },
      "outputs": [],
      "source": [
        "state = initial\n",
        "visited.add(initial)\n",
        "moves = 0\n",
        "while state != goal:\n",
        "    state, result = pick_move(state)\n",
        "    print(state, result)\n",
        "    visited.add(state)\n",
        "    moves += 1\n",
        "print(f\"moves: {moves}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}